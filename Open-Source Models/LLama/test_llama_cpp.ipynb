{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO ID                                 REPO TYPE SIZE ON DISK NB FILES LAST_ACCESSED LAST_MODIFIED REFS LOCAL PATH                                                                    \n",
      "--------------------------------------- --------- ------------ -------- ------------- ------------- ---- ----------------------------------------------------------------------------- \n",
      "Crystalcareai/GemMoE-Beta-1             model            21.7M        5 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--Crystalcareai--GemMoE-Beta-1             \n",
      "LanguageBind/MoE-LLaVA-StableLM-1.6B-4e model            445.0        1 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--LanguageBind--MoE-LLaVA-StableLM-1.6B-4e \n",
      "TheBloke/Llama-2-7b-Chat-GGUF           model             4.7G        1 1 hour ago    1 hour ago    main /data/shared/huggingface/hub/models--TheBloke--Llama-2-7b-Chat-GGUF           \n",
      "ToolBench/ToolLLaMA-7b                  model            27.0G        9 1 day ago     1 day ago     main /data/shared/huggingface/hub/models--ToolBench--ToolLLaMA-7b                  \n",
      "YanweiLi/MGM-2B                         model            21.7M        5 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--YanweiLi--MGM-2B                         \n",
      "bert-base-uncased                       model           441.1M        5 2 days ago    3 weeks ago   main /data/shared/huggingface/hub/models--bert-base-uncased                        \n",
      "codellama/CodeLlama-7b-Python-hf        model            25.8K        2 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--codellama--CodeLlama-7b-Python-hf        \n",
      "facebook/dinov2-large                   model             1.2G        3 3 weeks ago   3 weeks ago   main /data/shared/huggingface/hub/models--facebook--dinov2-large                   \n",
      "facebook/rag-sequence-nq                model             2.1G        9 2 hours ago   2 hours ago   main /data/shared/huggingface/hub/models--facebook--rag-sequence-nq                \n",
      "facebook/sam-vit-huge                   model             2.6G        3 3 weeks ago   3 weeks ago   main /data/shared/huggingface/hub/models--facebook--sam-vit-huge                   \n",
      "google/electra-large-discriminator      model             1.3G        5 3 weeks ago   3 weeks ago   main /data/shared/huggingface/hub/models--google--electra-large-discriminator      \n",
      "google/gemma-2b-it                      model             5.0G        9 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--google--gemma-2b-it                      \n",
      "google/gemma-7b                         model            17.1G       10 5 days ago    5 days ago    main /data/shared/huggingface/hub/models--google--gemma-7b                         \n",
      "huggyllama/llama-7b                     model            13.5G        9 1 day ago     1 day ago     main /data/shared/huggingface/hub/models--huggyllama--llama-7b                     \n",
      "jetmoe/jetmoe-8b                        model             2.3M        5 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--jetmoe--jetmoe-8b                        \n",
      "llama-moe/LLaMA-MoE-v1-3_5B-2_8         model           510.1K        5 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--llama-moe--LLaMA-MoE-v1-3_5B-2_8         \n",
      "lukeleeai/t5-base_moe_ex8               model             1.4G        6 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--lukeleeai--t5-base_moe_ex8               \n",
      "meta-llama/Llama-2-13b-chat-hf          model            26.0G       10 2 weeks ago   2 weeks ago   main /data/shared/huggingface/hub/models--meta-llama--Llama-2-13b-chat-hf          \n",
      "meta-llama/Llama-2-7b-hf                model            13.5G        9 4 days ago    2 weeks ago   main /data/shared/huggingface/hub/models--meta-llama--Llama-2-7b-hf                \n",
      "meta-llama/Meta-Llama-3-8B              model            16.1G       10 2 weeks ago   3 weeks ago   main /data/shared/huggingface/hub/models--meta-llama--Meta-Llama-3-8B              \n",
      "meta-llama/Meta-Llama-3-8B-Instruct     model            16.1G        6 4 weeks ago   4 weeks ago   main /data/shared/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct     \n",
      "mlabonne/phixtral-2x2_8                 model             8.9G       10 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--mlabonne--phixtral-2x2_8                 \n",
      "nvidia/Llama3-ChatQA-1.5-8B             model            16.1G        8 3 hours ago   4 hours ago   main /data/shared/huggingface/hub/models--nvidia--Llama3-ChatQA-1.5-8B             \n",
      "nvidia/dragon-multiturn-context-encoder model           438.0M        2 3 hours ago   4 hours ago   main /data/shared/huggingface/hub/models--nvidia--dragon-multiturn-context-encoder \n",
      "nvidia/dragon-multiturn-query-encoder   model           438.2M        5 4 hours ago   4 hours ago   main /data/shared/huggingface/hub/models--nvidia--dragon-multiturn-query-encoder   \n",
      "openbmb/Eurus-RM-7b                     model            14.2G        9 2 weeks ago   2 weeks ago   main /data/shared/huggingface/hub/models--openbmb--Eurus-RM-7b                     \n",
      "openbmb/MiniCPM-MoE-8x2B                model            27.8G        8 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--openbmb--MiniCPM-MoE-8x2B                \n",
      "sentence-transformers/all-MiniLM-L6-v2  model            91.6M       11 1 day ago     3 weeks ago   main /data/shared/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2  \n",
      "sfairXC/FsfairX-LLaMA3-RM-v0.1          model            15.0G        9 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--sfairXC--FsfairX-LLaMA3-RM-v0.1          \n",
      "xverse/XVERSE-MoE-A4.2B                 model             2.1M        3 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--xverse--XVERSE-MoE-A4.2B                 \n",
      "yzzh/gemma2b_v2                         model             5.0G        8 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--yzzh--gemma2b_v2                         \n",
      "yzzh/gemma2b_v3                         model             5.0G        4 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--yzzh--gemma2b_v3                         \n",
      "yzzh/gemma2b_v4                         model             5.0G        4 4 days ago    4 days ago    main /data/shared/huggingface/hub/models--yzzh--gemma2b_v4                         \n",
      "yzzh/gemma_moe_2bx3                     model            10.0G       10 3 days ago    3 days ago    main /data/shared/huggingface/hub/models--yzzh--gemma_moe_2bx3                     \n",
      "\n",
      "Done in 0.0s. Scanned 34 repo(s) for a total of \u001b[1m\u001b[31m256.0G\u001b[0m.\n",
      "\u001b[90mGot 29 warning(s) while scanning. Use -vvv to print details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install llama-cpp-python\n",
    "# !git clone https://github.com/ggerganov/llama.cpp\n",
    "# !pip install -U \"huggingface_hub[cli]\"\n",
    "#!huggingface-cli download TheBloke/Llama-2-7b-Chat-GGUF llama-2-7b-chat.Q5_K_S.gguf .\n",
    "!huggingface-cli scan-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_cpp import Llama\n",
    "\n",
    "# # model_kwargs = {\n",
    "# #   \"n_ctx\":4096,    # Context length to use\n",
    "# #   \"n_threads\":4,   # Number of CPU threads to use\n",
    "# #   \"n_gpu_layers\":-1,# Number of model layers to offload to GPU. Set to 0 if only using CPU\n",
    "# # }\n",
    "\n",
    "# llama_model = Llama(model_path=\"/data/prathish/Llama_train/llama.cpp/models/models--TheBloke--Llama-2-7b-Chat-GGUF/snapshots/191239b3e26b2882fb562ffccdd1cf0f65402adb/llama-2-7b-chat.Q5_K_S.gguf\")#, **model_kwargs)\n",
    "\n",
    "# output = llama_model(\n",
    "#       \"Q: What is causal inference in machine Leaning? A: \", # Prompt\n",
    "#       max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "#       stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "#       echo=True )\n",
    "# print(output)\n",
    "\n",
    "# prompt = \"Q: You are an expert in causal inference. The following question is not a typical commonsense query, but rather a meticulously designed question created by a professor specializing in causal inference, intended to assess the students' mastery of the course content.Imagine a self-contained, hypothetical world with only the following conditions, and without any unmentioned factors or causal relationships: Husband has a direct effect on wife and alarm clock. Wife has a direct effect on alarm clock. For husbands that don't set the alarm and wives that don't set the alarm, the probability of ringing alarm is 8%. For husbands that don't set the alarm and wives that set the alarm, the probability of ringing alarm is 54%. For husbands that set the alarm and wives that don't set the alarm, the probability of ringing alarm is 41%. For husbands that set the alarm and wives that set the alarm, the probability of ringing alarm is 86%. For husbands that don't set the alarm, the probability of alarm set by wife is 74%. For husbands that set the alarm, the probability of alarm set by wife is 24%. If we disregard the mediation effect through wife, would husband positively affect alarm clock? Answer only Yes or No A:\"\n",
    "# max_tokens = 400\n",
    "# temperature = 0.3\n",
    "# top_p = 0.1\n",
    "# echo = True\n",
    "# stop = [\"Q\", \"\\n\"]\n",
    "\n",
    "\n",
    "# # Define the parameters\n",
    "# model_output = llama_model(\n",
    "#        prompt,\n",
    "#        max_tokens=max_tokens,\n",
    "#        temperature=temperature,\n",
    "#        top_p=top_p,\n",
    "#        echo=echo,\n",
    "#        stop=stop,\n",
    "#    )\n",
    "# final_result = model_output[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /data/shared/huggingface/hub/models--TheBloke--claude2-alpaca-7B-GGUF/snapshots/73099953ac68a3b9d3d8004bd5f2debc2986cd5b/claude2-alpaca-7b.Q2_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32001]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32001]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32001]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 260/32001 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32001\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2694.32 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.model': 'llama', 'llama.context_length': '4096', 'general.architecture': 'llama', 'general.file_type': '10', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'general.name': 'LLaMA v2', 'llama.attention.head_count_kv': '32', 'llama.feed_forward_length': '11008', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.layer_norm_rms_epsilon': '0.000010'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "#my_model_path = \"/data/shared/huggingface/hub/models--TheBloke--Llama-2-7b-Chat-GGUF/snapshots/191239b3e26b2882fb562ffccdd1cf0f65402adb/llama-2-7b-chat.Q5_K_S.gguf\"\n",
    "CONTEXT_SIZE = 512\n",
    "my_model_path = \"/data/shared/huggingface/hub/models--TheBloke--claude2-alpaca-7B-GGUF/snapshots/73099953ac68a3b9d3d8004bd5f2debc2986cd5b/claude2-alpaca-7b.Q2_K.gguf\"\n",
    "\n",
    "\n",
    "# LOAD THE MODEL\n",
    "llama_model = Llama(model_path=my_model_path,\n",
    "                    n_ctx=CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_prompt(prompt,\n",
    "                             max_tokens = 500,\n",
    "                             temperature = 0.3,\n",
    "                             top_p = 0.1,\n",
    "                             echo = True):\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "   # Define the parameters\n",
    "        model_output = llama_model(\n",
    "            prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            echo=echo\n",
    "        )\n",
    "\n",
    "\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   29552.33 ms\n",
      "llama_print_timings:      sample time =       5.26 ms /    28 runs   (    0.19 ms per token,  5326.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   29551.85 ms /   324 tokens (   91.21 ms per token,    10.96 tokens per second)\n",
      "llama_print_timings:        eval time =    3635.75 ms /    27 runs   (  134.66 ms per token,     7.43 tokens per second)\n",
      "llama_print_timings:       total time =   33212.52 ms /   351 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-6d5114c2-648c-4be0-93a4-248cd290eb1a',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1717240321,\n",
       " 'model': '/data/shared/huggingface/hub/models--TheBloke--claude2-alpaca-7B-GGUF/snapshots/73099953ac68a3b9d3d8004bd5f2debc2986cd5b/claude2-alpaca-7b.Q2_K.gguf',\n",
       " 'choices': [{'text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\nYou are an expert in causal inference. The following question is not a typical commonsense query, but rather a meticulously designed question created by a professor specializing in causal inference, intended to assess the students' mastery of the course content. Imagine a self-contained, hypothetical world with only the following conditions, and without any unmentioned factors or causal relationships: Husband has a direct effect on wife and alarm clock. Wife has a direct effect on alarm clock. For husbands that don't set the alarm and wives that don't set the alarm, the probability of ringing alarm is 8%. For husbands that don't set the alarm and wives that set the alarm, the probability of ringing alarm is 54%. For husbands that set the alarm and wives that don't set the alarm, the probability of ringing alarm is 41%. For husbands that set the alarm and wives that set the alarm, the probability of ringing alarm is 86%. For husbands that don't set the alarm, the probability of alarm set by wife is 74%. For husbands that set the alarm, the probability of alarm set by wife is 24%. If we disregard the mediation effect through wife, would husband positively affect alarm clock? Start your answer with 'Yes' or 'No'\\n\\n### Yes\\n\\nIf we disregard the mediation effect through wife, then husband will positively affect alarm clock.\",\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 324, 'completion_tokens': 27, 'total_tokens': 351}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"You are an expert in causal inference. The following question is not a typical commonsense query, but rather a meticulously designed question created by a professor specializing in causal inference, intended to assess the students' mastery of the course content. Imagine a self-contained, hypothetical world with only the following conditions, and without any unmentioned factors or causal relationships: Husband has a direct effect on wife and alarm clock. Wife has a direct effect on alarm clock. For husbands that don't set the alarm and wives that don't set the alarm, the probability of ringing alarm is 8%. For husbands that don't set the alarm and wives that set the alarm, the probability of ringing alarm is 54%. For husbands that set the alarm and wives that don't set the alarm, the probability of ringing alarm is 41%. For husbands that set the alarm and wives that set the alarm, the probability of ringing alarm is 86%. For husbands that don't set the alarm, the probability of alarm set by wife is 74%. For husbands that set the alarm, the probability of alarm set by wife is 24%. If we disregard the mediation effect through wife, would husband positively affect alarm clock? Start your answer with 'Yes' or 'No'\"\n",
    "\n",
    "# prompt = f\"\"\"[INST] <<SYS>>\n",
    "# You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "# <</SYS>>\n",
    "# {user_prompt}[/INST]\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "{user_prompt}\n",
    "\"\"\"\n",
    "\n",
    "llama_model_response = generate_text_from_prompt(prompt)\n",
    "\n",
    "\n",
    "llama_model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\nYou are an expert in causal inference. The following question is not a typical commonsense query, but rather a meticulously designed question created by a professor specializing in causal inference, intended to assess the students' mastery of the course content. Imagine a self-contained, hypothetical world with only the following conditions, and without any unmentioned factors or causal relationships: Husband has a direct effect on wife and alarm clock. Wife has a direct effect on alarm clock. For husbands that don't set the alarm and wives that don't set the alarm, the probability of ringing alarm is 8%. For husbands that don't set the alarm and wives that set the alarm, the probability of ringing alarm is 54%. For husbands that set the alarm and wives that don't set the alarm, the probability of ringing alarm is 41%. For husbands that set the alarm and wives that set the alarm, the probability of ringing alarm is 86%. For husbands that don't set the alarm, the probability of alarm set by wife is 74%. For husbands that set the alarm, the probability of alarm set by wife is 24%. If we disregard the mediation effect through wife, would husband positively affect alarm clock? Start your answer with 'Yes' or 'No'\\n\\n### Yes\\n\\nIf we disregard the mediation effect through wife, then husband will positively affect alarm clock.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result = llama_model_response[\"choices\"][0][\"text\"].strip()\n",
    "final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prefix2norm = {\n",
    "#         'Yes': 1,\n",
    "#         'No': 0\n",
    "#     }\n",
    "\n",
    "\n",
    "# final_result = llama_model_response[\"choices\"][0][\"text\"].strip()\n",
    "# parts = final_result.split(\"[/INST]\")\n",
    "# following_text = parts[1].strip()  # Remove any leading whitespace\n",
    "# first_word = following_text.split()[0]\n",
    "# value = first_word.rstrip('.!?,')\n",
    "\n",
    "# for prefix, norm in prefix2norm.items():\n",
    "#     if prefix.lower() == value.lower():\n",
    "#         print(norm)\n",
    "\n",
    "def convert_response_to_numeric(text_input):\n",
    "    prefix2norm = {\n",
    "        'Yes': 1,\n",
    "        'No': 0\n",
    "    }\n",
    "    \n",
    "    invalid = -1\n",
    "    sections = text_input.split('\\n\\n### ')\n",
    "    if len(sections) < 2:\n",
    "        return invalid\n",
    "    \n",
    "    response_with_excess = sections[1].split('\\n\\n')[0]\n",
    "    response = response_with_excess.strip()\n",
    "    \n",
    "    return prefix2norm.get(response, invalid)\n",
    "\n",
    "\n",
    "text_input = llama_model_response[\"choices\"][0][\"text\"].strip()\n",
    "out = convert_response_to_numeric(text_input)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\nYou are an expert in causal inference. The following question is not a typical commonsense query, but rather a meticulously designed question created by a professor specializing in causal inference, intended to assess the students' mastery of the course content. Imagine a self-contained, hypothetical world with only the following conditions, and without any unmentioned factors or causal relationships: Husband has a direct effect on wife and alarm clock. Wife has a direct effect on alarm clock. For husbands that don't set the alarm and wives that don't set the alarm, the probability of ringing alarm is 8%. For husbands that don't set the alarm and wives that set the alarm, the probability of ringing alarm is 54%. For husbands that set the alarm and wives that don't set the alarm, the probability of ringing alarm is 41%. For husbands that set the alarm and wives that set the alarm, the probability of ringing alarm is 86%. For husbands that don't set the alarm, the probability of alarm set by wife is 74%. For husbands that set the alarm, the probability of alarm set by wife is 24%. If we disregard the mediation effect through wife, would husband positively affect alarm clock? Start your answer with 'Yes' or 'No'[/INST]  Yes.\\nThe question you provided is a well-designed causal inference problem, and I'm glad to help you with it! To answer this question, we need to use the given information to estimate the causal effect of husband on alarm clock, while disregarding the mediation effect through wife.\\nFrom the table, we can see that when husb\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes.\n",
      "The question you provided is a well-designed causal inference problem, and I'm glad to help you with it! To answer this question, we need to use the given information to estimate the causal effect of husband on alarm clock, while disregarding the mediation effect through wife.\n",
      "From the table, we can see that when husb\n"
     ]
    }
   ],
   "source": [
    "prompt_end_marker = \"[/INST]  \"  # The prompt ends with this marker followed by two spaces\n",
    "start_index = final_result.find(prompt_end_marker) + len(prompt_end_marker)\n",
    "\n",
    "# Extract the model-generated text after the prompt\n",
    "model_output = final_result[start_index:].strip()  # Strip to remove any leading/trailing whitespace\n",
    "\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmfoundry-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
