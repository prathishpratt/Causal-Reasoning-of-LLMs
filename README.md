# Evaluating Causal Reasoning in Large Language Models: Do LLMs understand causality?

**Authors**: Aseem Dandgaval, Dinesh Karthikeyan, Nandita Sanjivi, Prathish Murugan

---

This study assesses the ability of Large Language Models (LLMs) to perform formal causal reasoning, employing the comprehensive CLadder dataset as a testbed. Despite their extensive application across various domains of natural language processing, LLMs' competence in handling formal causal queries has not been thoroughly explored. Our research aims to determine whether LLMs can interpret and respond to complex causal inquiries in a manner consistent with human reasoning paradigms.

We systematically evaluate LLMs' performance by analyzing their responses to a broad spectrum of causal questions, including associational, interventional, and counterfactual scenarios. This evaluation focuses on the accuracy with which these models generate responses that are not only correct but also demonstrate coherent and logical reasoning steps that mirror formal reasoning patterns expected in human cognition.

The findings from this study provide valuable insights into the capabilities and limitations of current LLMs regarding causal reasoning. These insights pave the way for targeted improvements in model training and can significantly influence the deployment of LLMs in real-world applications where accurate causal inference is essential.

---
